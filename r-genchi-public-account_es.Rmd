---
title: "**Análisis de texto de las Cuentas Públicas Participativas de la gestión de Gendarmería de Chile entre los años 2017 y 2019**"
author: "Fabián Álvarez / Roberto Rodríguez"
date: "09-11-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r 1_libraries, include=FALSE}
source("1_libraries.r")
```

# Introducción

Las Cuentas Públicas Participativas son mecanismos de diálogo abierto que vinculan a las autoridades de los órganos de la Administración del Estado con la ciudadanía y tienen como objetivo informar sobre la gestión de políticas públicas realizadas, generar un proceso de retro alimentación que permita recoger las inquietudes y aportes de quienes participen de éstas y dar respuesta organizada en plazos oportunos a las inquietudes surgidas en el proceso.

Para este análisis se aplican técnicas de análisis de texto en R a las cuentas públicas de Gendarmería de Chile correspondientes a los años 2018, 2019 y 2020 (gestión 2017, 2018 y 2019, respectivamente).

# Lectura y limpieza de archivos

Primero, leemos la fuente de los datos que, en nuestro caso, se trata de archivos en formato pdf.  

```{r 2_source_reading}
speech_2020 <- pdf_text("input/discurso_2020_2019.pdf")
speech_2019 <- pdf_text("input/discurso_2019_2018.pdf")
speech_2018 <- pdf_text("input/discurso_2018_2017.pdf")
```

Luego, eliminamos las primeras páginas de los discursos ya que contienen elementos que no forman parte de la cuenta pública. Lo mismo hacemos para las últimas páginas; en ambos casos, solo cuando corresponda.

```{r 2_source_removing_pages}
speech_2020 <- speech_2020 %>%
  .[-1:-2] %>% 
  .[-53:-56]
speech_2019 <- speech_2019 %>% 
  .[-1:-2]
speech_2018 <- speech_2018 %>% 
  .[-1]
```

Los objetos en cada discurso quedaron separados, por lo que los unimos en un solo objeto por cada discurso.

```{r 2_source_concatenating}
speech_2020 <- paste(speech_2020, collapse = " ")
speech_2019 <- paste(speech_2019, collapse = " ")
speech_2018 <- paste(speech_2018, collapse = " ")
```

Las *stopwords* o *palabras vacías* son aquellas palabras que no tienen significado por sí mismas.  Solo modifican o acompañan a otras, por lo cual debemos quitarlas para el análisis.  Para ello, tomamos las palabras desde el repositorio [AnaText](https://github.com/7PartidasDigital/AnaText/blob/master/datos/diccionarios/vacias.txt) que ya hemos descargado previamente.  Agregamos palabras adicionales que vayamos encontrando irrelevantes.

```{r 2_source_stopwords, message=FALSE}
stopwords_es <- read_csv("input/vacias.txt", col_names = TRUE)
my_stopwords <- tibble(palabra = c("mil", "millones", "año", "años", "chile", "dado", 
                                   "dar", "debido", "decir", "acerca", "pesos",
                                   "fin", "ser", "respecto", "debe", "gran", "tiene",
                                   "tienen", "puede", "ir", "hace"))
more_stopwords <- tibble(palabra = c("cdp", "cerrado", "gendarmería", "fecha", "período", 
                                     "cuenta", "informe", "viii", "monto", "diariamente",
                                     "diferentes", "impacta", "enfocar",
                                     "circuito", "sanitarias", "deberá", "tot", "c.e.t."))
```

## Expresiones Regulares y Palabras Vacías (*stopwords*)

Revisamos expresiones regulares y palabras vacías para descartar del documento.

### Discurso 2020 (Gestión 2019)

Comenzamos con las expresiones regulares para el Discurso 2020.

```{r 3_cleaning1_regular_expressions_2020_2019}
speech_2020 <- speech_2020 %>%
  str_remove_all("\"") %>% # Remove "\"" 
  str_replace_all("\r\n", " ") %>% # Replace "\r\n" by space
  str_replace_all("MINJU-DDHH", "MINJUDDHH") %>% # Ministerio de Justicia y DDHH
  str_replace_all("MINJU,", "MINJUDDHH") %>% 
  str_replace_all("(MINJU)", "MINJUDDHH") %>% 
  str_remove_all("\\s+•\\s+") %>% # Bullets
  str_remove_all("http\\S*") %>% # urls
  str_remove_all("www.\\S*") %>% # Remove web pages
  str_replace_all("\\s+\\-+\\s", " ") %>% # Bullets
  str_remove_all("\\d+-\\s") %>%  # Numbering
  str_replace_all("\\s+\\d+\\.+\\s", " ") %>% # Replace 1. by space
  str_replace_all("\\s+\\d+\\.+\\d+\\.+\\s", " ") %>% # Replace 1.2. by space
  str_remove_all("\\s+[abcde]+\\)+\\s") %>% # Remove type of numbering a)  
  str_replace_all("\\s+\\(+\\d+\\)+\\s", " ") %>% # Replace type of numbering (a) by space
  str_replace_all("\\S+\\.+\\-", " ") %>% # Replace something like + . + - by space 
  str_remove_all("\\s+www.escueladegendarmeria.gob.cl.+cl.") %>%  # Remove web pages and social networks
  str_remove_all("\\s+N° de internos heridos.+Fuente: Sistema de información para la Gestión") %>% # Remove Tables
  str_remove_all("\\s+se expone un desglose y departamento respectivo:.+Fuente: Departamento de Infraestructura") %>% # Remove Tables 
  str_remove_all("\\s+MATRICULADOS EN EDUCACIÓN SUPERIOR DICIEMBRE 2019.+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Tabla 1. Privados de Libertad Inscritos para dar PSU.+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Tabla 2. Resultados PSU 2019 de Privados de Libertad, por región:.+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Intervención:.+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Prestaciones.+Fuente: Departamento Subsistema Cerrado") %>% # Remove Tables   
  str_remove_all("\\s+CANTIDAD DE CELULARES.+Fuente: Subdirección Operativa") %>% # Remove Tables 
  str_remove_all("\\s+18. COVID: Estadística de contagios por región.+Fuente: Subdirección Operativa") %>% # Remove Tables 
  str_remove_all("\\s+Catastro de beneficios otorgados.+Fuente: Subdirección Operativa") %>% # Remove Tables 
  stripWhitespace() # Remove unnecessary spaces
speech_2020 <- gsub("COVID 19", "COVID", speech_2020) # Standardize COVID-19
speech_2020 <- gsub("COVID- 19", "COVID", speech_2020) # Standardize COVID-19
```

Convertimos el discurso en un dataframe, separamos sus palabras y calculamos sus frecuencias.

```{r 3_cleaning1_first_frequencies_2020_2019}
frequencies_2020 <- tibble(speech = speech_2020) %>% 
  unnest_tokens(output = palabra, input = speech, strip_numeric = TRUE) %>%
  count(palabra, sort = TRUE)
frequencies_2020
```

Quitamos las *stopwords* y recalculamos las frecuencias.

```{r 3_cleaning1_second_frequencies_2020_2019, message=FALSE}
frequencies_2020 <- frequencies_2020 %>% 
  anti_join(stopwords_es) %>% 
  anti_join(my_stopwords) %>% 
  anti_join(more_stopwords)
head(frequencies_2020)
```

### Discurso 2019 (Gestión 2018)

Comenzamos con las expresiones regulares para el Discurso 2019.

```{r 3_cleaning2_regular_expressions_2019_2018}
speech_2019 <- speech_2019 %>% 
  str_remove_all("\r\n+\\s+\\d+\r\n") %>%  # Page number  
  str_replace_all("\r\n", " ") %>%
  str_replace_all("MINJU-DDHH", "MINJUDDHH") %>% # Ministerio de Justicia y DDHH
  str_replace_all("MINJU,", "MINJUDDHH") %>% 
  str_replace_all("(MINJU)", "MINJUDDHH") %>% 
  str_remove_all("\\s+•\\s+") %>% # Bullets
  str_remove_all("\\s+\\-+\\s") %>% # Bullets  
  str_remove_all("\\s+Tabla relación entre programas y objetivos principales definidos.+Fuente: Departamento de Infraestructura de Gendarmería de Chile") %>%   
  str_remove_all("\\s+Capacitaciones en cifras.+Fuente: Escuela Institucional") %>% 
  str_remove_all("\\s+Presupuesto Inicial.+Fuente: Departamento de Contabilidad y Presupuesto, Gendarmería de Chile") %>% 
  str_replace_all("N°+\\s+\\d+\\.+\\s", " ") %>%  # N° 1.
  str_replace_all("\\s+\\d+\\.+\\s", " ") %>% # 1.
  str_replace_all("\\s+\\d+\\.+\\d+\\.+\\s", " ") %>%  # 4.1.
  str_replace_all("\\s+\\d+\\.+\\d+\\.+\\d+\\.+\\s", " ") %>%  # 4.1.1.  
  stripWhitespace()
```

Convertimos el discurso en un dataframe, separamos sus palabras y calculamos sus frecuencias.

```{r 3_cleaning2_first_frequencies_2019_2018}
frequencies_2019 <- tibble(speech = speech_2019) %>% 
  unnest_tokens(output = palabra, input = speech, strip_numeric = TRUE) %>%
  count(palabra, sort = TRUE)
frequencies_2019
```

Quitamos las *stopwords* y recalculamos las frecuencias.

```{r 3_cleaning2_second_frequencies_2019_2018, message=FALSE}
frequencies_2019 <- frequencies_2019 %>% 
  anti_join(stopwords_es) %>% 
  anti_join(my_stopwords) %>% 
  anti_join(more_stopwords)
head(frequencies_2019)
```

### Discurso 2018 (Gestión 2017)

Comenzamos con las expresiones regulares para el Discurso 2018.

```{r 3_cleaning3_regular_expressions_2018_2017}
speech_2018 <- speech_2018 %>% 
  str_replace_all("\r\n", " ") %>%
  str_replace_all("MINJU-DDHH", "MINJUDDHH") %>% # Ministerio de Justicia y DDHH
  str_replace_all("MINJU,", "MINJUDDHH") %>% 
  str_replace_all("(MINJU)", "MINJUDDHH") %>% 
  str_remove_all("\\s+•\\s+") %>% # Bullets
  str_remove_all("\\s+\\-+\\s") %>% # Bullets  
  str_remove_all("\\s+Allanamientos.+7+\\.+854") %>%   
  str_remove_all("\\s+Establecimientos Penitenciarios.+43") %>% 
  str_remove_all("\\s+Para el año 2018 se desarrollarán.+305") %>% 
  str_remove_all("\\s+N° de horas.+\\(+LA ARAUCANÍA+\\)") %>%   
  str_remove_all("\\s+Los beneficiados de los programas.+DIPLOMADO GERENCIA PUBLICA+\\s+\\d") %>%     
  str_replace_all("\\s+\\d+\\.+\\s", " ") %>%  # 1. 
  str_replace_all("I+\\.+\\s+AVANCES", "AVANCE") %>%
  str_replace_all("\\s+II+\\.+\\s", " ") %>%  
  stripWhitespace() 
```

Convertimos el discurso en un dataframe, separamos sus palabras y calculamos sus frecuencias.

```{r 3_cleaning3_first_frequencies_2018_2017}
frequencies_2018 <- tibble(speech = speech_2018) %>% 
  unnest_tokens(output = palabra, input = speech, strip_numeric = TRUE) %>%
  count(palabra, sort = TRUE)
frequencies_2018
```

Quitamos las *stopwords* y recalculamos las frecuencias.

```{r 3_cleaning3_second_frequencies_2018_2017, message=FALSE}
frequencies_2018 <- frequencies_2018 %>% 
  anti_join(stopwords_es) %>% 
  anti_join(my_stopwords) %>% 
  anti_join(more_stopwords)
head(frequencies_2018)
```

# Análisis por palabra

Primero calculamos y graficamos las palabras más frecuentes en cada discurso para, posteriormente, realizar un análisis TF-IDF.  Este análisis permite determinar la relevancia de una palabra para un documento respecto de un conjunto de documentos.

## Frecuencias

Graficamos las 10 palabras más frecuentes de cada discurso.

```{r 4_one-word_analysis_frequencies}
pic_2020 <- frequencies_2020 %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(y = reorder(palabra, n), n)) +
  geom_col(fill = "#619cff") +
  geom_text(aes(label = n), size = 3, hjust = -0.5) +
  theme_minimal() +
  labs(y = NULL, x = "frecuencia") +
  ggtitle("Discurso 2020") +
  theme(plot.title = element_text(hjust = 0.5))

pic_2019 <- frequencies_2019 %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(y = reorder(palabra, n), n)) +
  geom_col(fill = "#00ba38") +
  geom_text(aes(label = n), size = 3, hjust = 0.2) +
  theme_minimal() +
  labs(y = NULL, x = "frecuencia") +
  ggtitle("Discurso 2019") +
  theme(plot.title = element_text(hjust = 0.5))

pic_2018 <- frequencies_2018 %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(y = reorder(palabra, n), n)) +
  geom_col(fill = "#f8766d") +
  geom_text(aes(label = n), size = 3, hjust = 0.2) +
  theme_minimal() +
  labs(y = NULL, x = "frecuencia") +
  ggtitle("Discurso 2018") +
  theme(plot.title = element_text(hjust = 0.5))

(pic_2018 + pic_2019) / pic_2020 
```

## Análisis TF-IDF

Unimos las frecuencias en un solo data frame, identificando palabras y frecuencias con sus respectivos discursos.

```{r 4_one-word_analysis_TF-IDF-one_data_frame}
frequencies_2020 <- frequencies_2020 %>% 
  mutate(discurso = "C.P.P. 2020", .before = palabra) 
frequencies_2019 <- frequencies_2019 %>% 
  mutate(discurso = "C.P.P. 2019", .before = palabra) 
frequencies_2018 <- frequencies_2018 %>% 
  mutate(discurso = "C.P.P. 2018", .before = palabra) 

messages <- bind_rows(frequencies_2018, frequencies_2019, frequencies_2020)
head(messages)
```

Luego, calculamos las frecuencias inversas de las palabras.

```{r 4_one-word_analysis_TF-IDF_inverse_document_frequency}
messages_tfidf <- bind_tf_idf(messages, 
                              term = palabra, 
                              document = discurso,
                              n = n)
head(messages_tfidf) 
```

Y comparamos gráficamente.

```{r 4_one-word_analysis_TF-IDF_plotting_TF-IDF, message=FALSE}
messages_tfidf %>%
  group_by(discurso) %>%
  top_n(9) %>%
  ungroup %>%
  mutate(discurso = as.factor(discurso),
         palabra = reorder_within(palabra, tf_idf, discurso)) %>%
  ggplot(aes(palabra, tf_idf, fill = discurso)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~discurso, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0,0)) +
  labs(y = "tf-idf", x = NULL)
```

De las gráficas anteriores, vemos que el término *covid* se tomó la atención del discurso 2020, lo cual tiene mucho sentido en atención a la emergencia sanitaria de pandemia que estamos viviendo a la fecha. Similarmente, esto ocurre para los términos *pandemia* y *contagios*.


