---
title: "**Análisis de texto de las Cuentas Públicas Participativas de la gestión de Gendarmería de Chile entre los años 2017 y 2019**"
author: "Fabián Álvarez / Roberto Rodríguez"
date: "09-11-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r 1_libraries, include=FALSE}
source("1_libraries.r")
```

# Introducción

Las Cuentas Públicas Participativas son mecanismos de diálogo abierto que vinculan a las autoridades de los órganos de la Administración del Estado con la ciudadanía y tienen como objetivo informar sobre la gestión de políticas públicas realizadas, generar un proceso de retro alimentación que permita recoger las inquietudes y aportes de quienes participen de éstas y dar respuesta organizada en plazos oportunos a las inquietudes surgidas en el proceso.

Para este análisis se aplican técnicas de análisis de texto en R a las cuentas públicas de Gendarmería de Chile correspondientes a los años 2018, 2019 y 2020 (gestión 2017, 2018 y 2019, respectivamente).

# Lectura y limpieza de archivos

Primero, leemos la fuente de los datos que, en nuestro caso, se trata de archivos en formato pdf.  

```{r 2_source_reading}
speech_2020 <- pdf_text("input/discurso_2020_2019.pdf")
speech_2019 <- pdf_text("input/discurso_2019_2018.pdf")
speech_2018 <- pdf_text("input/discurso_2018_2017.pdf")
```

Luego, eliminamos las primeras páginas de los discursos ya que contienen elementos que no forman parte de la cuenta pública. Lo mismo hacemos para las últimas páginas; en ambos casos, solo cuando corresponda.

```{r 2_source_removing_pages}
speech_2020 <- speech_2020 %>%
  .[-1:-2] %>% 
  .[-53:-56]
speech_2019 <- speech_2019 %>% 
  .[-1:-2]
speech_2018 <- speech_2018 %>% 
  .[-1]
```

Los objetos en cada discurso quedaron separados, por lo que los unimos en un solo objeto por cada discurso.

```{r 2_source_concatenating}
speech_2020 <- paste(speech_2020, collapse = " ")
speech_2019 <- paste(speech_2019, collapse = " ")
speech_2018 <- paste(speech_2018, collapse = " ")
```

Las *stopwords* o *palabras vacías* son aquellas palabras que no tienen significado por sí mismas.  Solo modifican o acompañan a otras, por lo cual debemos quitarlas para el análisis.  Para ello, tomamos las palabras desde el repositorio [AnaText](https://github.com/7PartidasDigital/AnaText/blob/master/datos/diccionarios/vacias.txt) que ya hemos descargado previamente.  Agregamos palabras adicionales que vayamos encontrando irrelevantes.

```{r 2_source_stopwords, message=FALSE}
stopwords_es <- read_csv("input/vacias.txt", col_names = TRUE)
my_stopwords <- tibble(palabra = c("mil", "millones", "año", "años", "chile", "dado", 
                                   "dar", "debido", "decir", "acerca", "pesos",
                                   "fin", "ser", "respecto", "debe", "gran", "tiene",
                                   "tienen", "puede", "ir", "hace"))
more_stopwords <- tibble(palabra = c("cdp", "cerrado", "gendarmería", "fecha", "período", 
                                     "cuenta", "informe", "viii", "monto", "diariamente",
                                     "diferentes", "impacta", "enfocar", "deberá"))
```

## Expresiones Regulares y Palabras Vacías (*stopwords*)

Revisamos expresiones regulares y palabras vacías para descartar del documento.

### Discurso 2020 (Gestión 2019)

Comenzamos eliminando las expresiones regulares para el Discurso 2020 y haciendo algunas correcciones.

```{r 3_cleaning1_regular_expressions_2020_2019}
speech_2020 <- speech_2020 %>%
  str_replace_all("\n", " ") %>% # Replace "\n" by space
  str_remove_all("“") %>% str_remove_all("”") %>% # Remove "" 
  str_replace_all("MINJU", "MINJUDDHH") %>% # Ministerio de Justicia y DDHH
  str_replace_all("MINJUDDHH-DDHH", "MINJUDDHH") %>% 
  str_replace_all("\\s+•\\s+", " ") %>% # Bullets
  str_replace_all("\\s\\S+\\.-\\s", " ") %>% 
  str_replace_all("\\s\\d-\\s", " ") %>% # Numbering
  str_replace_all("\\s\\d\\.\\s", " ") %>% 
  str_replace_all("\\s\\d\\d\\.\\s", " ") %>% 
  str_replace_all("\\s\\d\\.\\d\\s", " ") %>% 
  str_replace_all("\\s+[abcde]+\\)+\\s", " ") %>% 
  str_replace_all("\\s[abcde]\\.\\d\\)", " ") %>% 
  str_replace_all("19\\.-", " ") %>% # 
  str_remove_all("http\\S*") %>% # urls
  str_remove_all("www.\\S*") %>% # Remove web pages
  str_remove_all("Twitter.+gendarmeriacl") %>%  # Remove social networks
  str_remove_all("N° de internos heridos.+\\(S\\.I\\.G\\)") %>% # Remove Tables
  str_remove_all("A continuación, se expone un desglose.+Fuente: Departamento de Infraestructura") %>% # Remove Tables 
  str_remove_all("MATRICULADOS EN EDUCACIÓN SUPERIOR DICIEMBRE 2019.+Total\\s+163\\s+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Tabla Privados de Libertad Inscritos para dar PSU.+\\s+2046\\s+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Tabla Resultados PSU 2019 de Privados de Libertad, por región:.+\\s+13\\s+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+INTERNOS PARTICIPANDO.+\\s+Automotriz\\s+Fuente: Departamento Sistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+Eliminación de antecedentes:.+Fuente: Departamento Post Penitenciario") %>% # Remove Tables 
  str_remove_all("\\s+Intervención:.+\\s+37\\s+Fuente: Departamento Subsistema Cerrado") %>% # Remove Tables 
  str_remove_all("\\s+CANTIDAD DE CELULARES.+\\s+256\\s+Fuente: Subdirección Operativa") %>% # Remove Tables 
  str_remove_all("\\sCOVID: Estadística de contagios por región.+\\s+1357\\s+Fuente: Subdirección Operativa") %>% # Remove Tables 
  str_remove_all("\\s+Catastro.+\\s+167\\s+Fuente: Subdirección Operativa") %>% # Remove Tables 
  str_remove_all("\\sfecha:.+Fuente: Subdirección Operativa") %>% # Remove Tables 
  stripWhitespace() # Remove unnecessary spaces
```

```{r 3_cleaning1_additional_corrections}
speech_2020 <- speech_2020 %>% 
  str_replace_all("COVID\\s19", "COVID-19") %>% # Standardize COVID-19
  str_replace_all("COVID-\\s19", "COVID-19") %>% 
  str_replace_all("COVID:", "COVID-19") %>% 
  str_replace_all("Covid-19", "COVID-19") %>% 
  str_replace_all("Covid19", "COVID-19") %>% 
  str_replace_all("COVID y", "COVID-19 y") %>% 
  str_replace_all("cas2", "cas") %>% # Others
  str_replace_all("ransparencia\\.", "ransparencia ") %>% 
  str_replace_all("para la transparencia", "para la Transparencia") %>% 
  str_replace_all("s e g u r i d a d in t e r na", "seguridad interna") %>% 
  str_replace_all("d e l o s re c i n to s", "de los recintos") %>% 
  str_replace_all("p e n i te nc i a r io s", "penitenciarios") %>% 
  str_remove_all("Departamento Sistema Cerrado") %>% 
  str_remove_all("Departamento de Salud") %>% 
  str_remove_all("Departamento de DDHH") %>% 
  str_remove_all("Departamento DDHH") %>% 
  str_remove_all("Departamento de Promoción y Protección de los DDHH") %>% 
  str_remove_all("Departamento de Promoción y Protección de Derechos Humanos") %>% 
  str_remove_all("Departamento de Promoción y Protección de los Derechos Humanos") %>% 
  str_remove_all("Departamento de Infraestructura") %>% 
  str_remove_all("Departamento de Informática") %>% 
  str_remove_all("Departamento en el Sistema Cerrado") %>% 
  str_remove_all("Departamento") %>% 
  str_replace_all("autoridades sanitarias", "autoridad sanitaria") %>% 
  str_replace_all("condiciones sanitarias", "condición sanitaria") %>% 
  str_replace_all("residencias sanitarias", "residencia sanitaria") %>% 
  str_replace_all("restricciones sanitarias", "restricción sanitaria") %>% 
  str_replace_all("sanitarias", "sanitaria") %>% 
  str_replace_all("Sanitarias", "sanitaria") %>% 
  str_replace_all("contagios", "contagio") %>% 
  stripWhitespace() 
```

Convertimos el discurso en un dataframe, separamos sus palabras y calculamos sus frecuencias.

```{r 3_cleaning1_first_frequencies_2020_2019}
frequencies_2020 <- tibble(speech = speech_2020) %>% 
  unnest_tokens(output = palabra, input = speech, strip_numeric = TRUE) %>%
  count(palabra, sort = TRUE)
frequencies_2020
```

Quitamos las *stopwords* y recalculamos las frecuencias.

```{r 3_cleaning1_second_frequencies_2020_2019, message=FALSE}
frequencies_2020 <- frequencies_2020 %>% 
  anti_join(stopwords_es) %>% 
  anti_join(my_stopwords) %>% 
  anti_join(more_stopwords)
head(frequencies_2020)
```

### Discurso 2019 (Gestión 2018)

Comenzamos eliminando las expresiones regulares para el Discurso 2019 y haciendo algunas correcciones.

```{r 3_cleaning2_regular_expressions_2019_2018}
speech_2019 <- speech_2019 %>% 
  str_replace_all("\n", " ") %>% # Replace "\n" by space
  str_remove_all("Tabla rela.+\\s+Fuente: Departamento de Infraestructura de Gendarmería de Chile") %>%  # Remove Tables
  str_remove_all("Capacitaciones en cifras.+\\s+Fuente: Escuela Institucional") %>% 
  str_remove_all("Presupuesto Inicial.+\\s+Contabilidad y Presupuesto, Gendarmería de Chile") %>% 
  str_replace_all("\\s\\s\\s\\s\\d+\\s", " ") %>% # Page number
  str_replace_all("\\s\\d\\.\\s", " ") %>% # Numbering & bullets
  str_remove_all("1. PRESENTACIÓN ") %>% 
  str_replace_all("\\s\\d\\.\\d\\.\\d\\.\\s", " ") %>% 
  str_replace_all("\\s\\d\\.\\d\\.\\s", " ") %>% 
  str_replace_all("\\s•\\s", " ") %>% 
  str_replace_all("\\s-\\s", " ") %>% 
  str_replace_all("\\s\\S\\.\\s", " ") %>% 
  str_replace_all("Ministerio de Justicia y Derechos Humanos", "MINJUDDHH") %>% 
  str_replace_all("Ministerio de Justicia y DD.HH.", "MINJUDDHH") %>% 
  str_remove_all("N°")
```

```{r 3_cleaning2_additional_corrections}
speech_2019 <- speech_2019 %>% 
  str_replace_all("ETIntervención", "Intervención") %>% 
  str_replace_all("ETCapacitación", "Capacitación") %>% 
  str_replace_all("ETColocación", "Colocación") %>% 
  str_replace_all("APpsicosocial", "psicosocial") %>% 
  str_replace_all("", " ") %>% 
  str_replace_all("APestablecimiento", "establecimiento") %>%
  str_replace_all("Aapoyo", "apoyo") %>%
  str_replace_all("Aopermisos", "permisos") %>%
  str_replace_all("1terceros", "terceros") %>% 
  str_remove_all("“") %>% str_remove_all("”") %>% 
  str_replace_all("de \\+R", "de Proyecto +R") %>% 
  str_replace_all("Departamento de Contabilidad y Presupuesto", " ") %>% 
  str_replace_all("Departamento de Gestión de Personas", " ") %>% 
  str_replace_all("Departamento de Gestión y Desarrollo de Personas", " ") %>% 
  str_replace_all("Departamento de Salud", " ") %>% 
  str_replace_all("Departamentos de Inteligencia Penitenciaria\\s+y de\\s+Investigación Criminal", " ") %>% 
  str_replace_all("Departamento de Investigación y Análisis Penitenciario \\(DIAP\\)", " ") %>% 
  str_replace_all("departamentos de Salud e Informática", " ") %>% 
  str_replace_all("Departamento de Promoción y Protección de los Derechos Humanos", " ") %>% 
  str_replace_all("Departamento de Infraestructura", " ") %>% 
  str_replace_all("Subdepartamento de Servicios\\s+Especializados", " ") %>% 
  str_replace_all("Departamento de Control Penitenciario", " ") %>% 
  str_replace_all("departamentos y/o unidades", " ") %>% 
  str_remove_all("a Departamento") %>% 
  str_remove_all("términos") %>% 
  str_remove_all("mejorando") %>% 
  str_remove_all("desarrollar") %>% 
  str_remove_all("porcentaje") %>% 
  stripWhitespace()
```

Convertimos el discurso en un dataframe, separamos sus palabras y calculamos sus frecuencias.

```{r 3_cleaning2_first_frequencies_2019_2018}
frequencies_2019 <- tibble(speech = speech_2019) %>% 
  unnest_tokens(output = palabra, input = speech, strip_numeric = TRUE) %>%
  count(palabra, sort = TRUE)
frequencies_2019
```

Quitamos las *stopwords* y recalculamos las frecuencias.

```{r 3_cleaning2_second_frequencies_2019_2018, message=FALSE}
frequencies_2019 <- frequencies_2019 %>% 
  anti_join(stopwords_es) %>% 
  anti_join(my_stopwords) %>% 
  anti_join(more_stopwords)
head(frequencies_2019)
```

### Discurso 2018 (Gestión 2017)

Comenzamos eliminando las expresiones regulares para el Discurso 2018 y haciendo algunas correcciones.

```{r 3_cleaning3_regular_expressions_2018_2017}
speech_2018 <- speech_2018 %>% 
  str_replace_all("\n", " ") %>% # Replace "\n" by space
  str_remove_all("\\s+Allanamientos\\s+.+854") %>% # Remove Tables
  str_remove_all("\\s+PROGRAMAS\\s+RE.+LACTANTES") %>% 
  str_remove_all("Distribución Regional.+NACIONAL\\s+43") %>% 
  str_remove_all("siguiente detalle.+Total\\s+305") %>% 
  str_remove_all("Aspirante Oficiales P.+327 Hombres\\)") %>% 
  str_remove_all("\\s+N° de horas.+ARAUCANÍA\\)") %>% 
  str_remove_all("AÑO NOMBRE.+PUBLICA 30")
```

```{r 3_cleaning3_additional_corrections}
speech_2018 <- speech_2018 %>% 
  str_remove_all("N°") %>% 
  str_replace_all("Departamento del Sistema Cerrado\\.", " ") %>% 
  str_replace_all("Departamento de Recursos Humanos", " ") %>% 
  str_replace_all("multidisclinarios", "multidisciplinarios") %>% 
  str_replace_all("asociadas:Implementación", "asociadas Implementación") %>% 
  str_replace_all("pesos\\.Reposición", "pesos Reposición") %>% 
  str_replace_all("incendios:Programa", "incendios Programa") %>% 
  str_replace_all("Coyhaique\\.Mantención", "Coyhaique Mantención") %>% 
  stripWhitespace() 
```

Convertimos el discurso en un dataframe, separamos sus palabras y calculamos sus frecuencias.

```{r 3_cleaning3_first_frequencies_2018_2017}
frequencies_2018 <- tibble(speech = speech_2018) %>% 
  unnest_tokens(output = palabra, input = speech, strip_numeric = TRUE) %>%
  count(palabra, sort = TRUE)
frequencies_2018
```

Quitamos las *stopwords* y recalculamos las frecuencias.

```{r 3_cleaning3_second_frequencies_2018_2017, message=FALSE}
frequencies_2018 <- frequencies_2018 %>% 
  anti_join(stopwords_es) %>% 
  anti_join(my_stopwords) %>% 
  anti_join(more_stopwords)
head(frequencies_2018)
```

# Análisis por palabra

Primero calculamos y graficamos las palabras más frecuentes en cada discurso para, posteriormente, realizar un análisis TF-IDF.  Este análisis permite determinar la relevancia de una palabra para un documento respecto de un conjunto de documentos.

## Frecuencias

Graficamos las 10 palabras más frecuentes de cada discurso.

```{r 4_one-word_analysis_frequencies}
pic_2020 <- frequencies_2020 %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(y = reorder(palabra, n), n)) +
  geom_col(fill = "#619cff") +
  geom_text(aes(label = n), size = 3, hjust = -0.5) +
  theme_minimal() +
  labs(y = NULL, x = "frecuencia") +
  ggtitle("Discurso 2020") +
  theme(plot.title = element_text(hjust = 0.5))

pic_2019 <- frequencies_2019 %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(y = reorder(palabra, n), n)) +
  geom_col(fill = "#00ba38") +
  geom_text(aes(label = n), size = 3, hjust = 0.2) +
  theme_minimal() +
  labs(y = NULL, x = "frecuencia") +
  ggtitle("Discurso 2019") +
  theme(plot.title = element_text(hjust = 0.5))

pic_2018 <- frequencies_2018 %>% 
  slice_head(n = 10) %>% 
  ggplot(aes(y = reorder(palabra, n), n)) +
  geom_col(fill = "#f8766d") +
  geom_text(aes(label = n), size = 3, hjust = 0.2) +
  theme_minimal() +
  labs(y = NULL, x = "frecuencia") +
  ggtitle("Discurso 2018") +
  theme(plot.title = element_text(hjust = 0.5))

(pic_2018 + pic_2019) / pic_2020 
```

## Análisis TF-IDF

Unimos las frecuencias en un solo data frame, identificando palabras y frecuencias con sus respectivos discursos.

```{r 4_one-word_analysis_TF-IDF-one_data_frame}
frequencies_2020 <- frequencies_2020 %>% 
  mutate(discurso = "C.P.P. 2020", .before = palabra) 
frequencies_2019 <- frequencies_2019 %>% 
  mutate(discurso = "C.P.P. 2019", .before = palabra) 
frequencies_2018 <- frequencies_2018 %>% 
  mutate(discurso = "C.P.P. 2018", .before = palabra) 

messages <- bind_rows(frequencies_2018, frequencies_2019, frequencies_2020)
head(messages)
```

Luego, calculamos las frecuencias inversas de las palabras.

```{r 4_one-word_analysis_TF-IDF_inverse_document_frequency}
messages_tfidf <- bind_tf_idf(messages, 
                              term = palabra, 
                              document = discurso,
                              n = n)
head(messages_tfidf) 
```

Y comparamos gráficamente.

```{r 4_one-word_analysis_TF-IDF_plotting_TF-IDF, message=FALSE}
messages_tfidf %>%
  group_by(discurso) %>%
  top_n(5) %>%
  ungroup %>%
  mutate(discurso = as.factor(discurso),
         palabra = reorder_within(palabra, tf_idf, discurso)) %>%
  ggplot(aes(palabra, tf_idf, fill = discurso)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~discurso, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0,0)) +
  labs(y = "tf-idf", x = NULL)
```

En el discurso del 2018 se dio énfasis a la actividad, capacitación y colocación *laboral*, y a aquellos *penados* que han sido intervenidos.  Asimismo, se rindió cuenta de *recursos* ejecutados, financiados en la Ley de Presupuesto.  También se mencionan *logros* en ámbitos de seguridad y custodia, reinserción social y gestión institucional.

Durante el 2019, el discurso se enfocó en destacar los programas de intervención, capacitación y colocación *laboral*, así como también las actividades *laborales* realizadas al interior de los establecimientos penitenciarios.  De la misma forma, se dio énfasis a la *vigilancia* de las personas privadas de libertad, la adopción de normas *internacionales* en el ámbito contable, buenas prácticas ajustadas al derecho *internacional* y el cumplimiento de compromisos con organismos *internacionales*.  También se menciona la realización de distintas actividades *culturales* con la participación de la población penal.

Por último, vemos que el término *covid* se tomó la atención del discurso 2020, lo cual tiene mucho sentido en atención a la emergencia *sanitaria* de *pandemia* que estamos viviendo a la fecha.  En este contexto, se implementaron *visitas* virtuales con programación previa y el acceso a teléfonos públicos para que los internos mantengan contacto con sus familiares.  También se puso atención a la gran cantidad de detenidos que eran recibidos con *lesiones* evidentes desde las instituciones policiales, para lo cual se aprobaron protocolos de constatación de estado de salud.









